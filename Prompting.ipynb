{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b218a5df",
   "metadata": {},
   "source": [
    "***\n",
    "<span style=\"font-size:32px; color:rgba(0, 0, 255, 0.5);\">Day 1 - Foundational Large Language Models & Text Generation</span>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c24e50-a0ec-4aa7-adc0-b6e7dcbd238f",
   "metadata": {},
   "source": [
    "<table style=\"width: 100%;\">\n",
    "  <tr>\n",
    "    <td style=\"background-color: rgba(0, 255, 0, 0.2); text-align: center; font-size: 16px;\">\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c7a305",
   "metadata": {},
   "source": [
    "<span style=\"font-size:24px; color:rgba(0, 0, 0, 0.5);\">Prompting</span>\n",
    "\n",
    "---\n",
    "\n",
    "\"The advent of Large Language Models (LLMs) represents a seismic shift in the world of artificial intelligence. Their ability to process, generate, and understand user intent is fundamentally changing the way we interact with information and technology.\n",
    "\n",
    "An LLM is an advanced artificial intelligence system that specializes in processing, understanding, and generating human-like text. These systems are typically implemented as a deep neural network and are trained on massive amounts of text data. This allows them to learn the intricate patterns of language, giving them the ability to perform a variety of tasks,\n",
    "like machine translation, creative text generation, question answering, text summarization, and many more reasoning and language oriented tasks. This whitepaper dives into the timeline of the various architectures and approaches building up to the large language models and the architectures being used at the time of publication. It also discusses fine-tuning techniques to customize an LLM to a certain domain or task, methods to make the training more efficient, as well as methods to accelerate inference. These are then followed by various applications and code examples.\"\n",
    "\n",
    "**Authors**<br>\n",
    "Mohammadamin Barektain, Anant Nawalgaria, Daniel J. Mankowitz, Majd Al Merey, Yaniv Leviathan, Massimo Mascaro, Matan Kalman, Elena Buchatskaya, Aliaksei Severyn, and Antonio Gulli "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb06a82",
   "metadata": {},
   "source": [
    "<span style=\"font-size:18px; color:rgba(0, 0, 0, 0.5);\">Resources</span>\n",
    "\n",
    "---\n",
    "**Whitepaper**<br>\n",
    "https://www.kaggle.com/whitepaper-foundational-llm-and-text-generation\n",
    "\n",
    "**Foundational LLM Podcast**<br>\n",
    "https://www.youtube.com/watch?v=mQDlCZZsOyo\n",
    "\n",
    "**Foundational Live Stream**<br>\n",
    "https://www.youtube.com/watch?v=kpRyiJUUFxY\n",
    "\n",
    "**Get your API key from**<br>\n",
    "https://aistudio.google.com/app/apikey\n",
    "\n",
    "**Kaggle**<br>\n",
    "https://www.kaggle.com/code/markishere/day-1-prompting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d781274d",
   "metadata": {},
   "source": [
    "<span style=\"font-size:18px; color:rgba(0, 0, 0, 0.5);\">Libraries</span>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f20d9ec-6b97-492e-bd11-a979f66b7bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch torchvision torchaudio tensorflow transformers google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e184186d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, enum, json, time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "import google.generativeai as genai\n",
    "from google.api_core import retry\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "from typing import TypedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037b66fd",
   "metadata": {},
   "source": [
    "<span style=\"font-size:18px; color:rgba(0, 0, 0, 0.5);\">Initialize the API</span>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22352c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API key from .env file\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GAI_API_KEY\")\n",
    "\n",
    "# Set up the API key for the genai library\n",
    "genai.configure(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c31fd8",
   "metadata": {},
   "source": [
    "<span style=\"font-size:18px; color:rgba(0, 0, 0, 0.5);\">Run your first prompt</span>\n",
    "\n",
    "---\n",
    "In this step, you will test that your API key is set up correctly by making a request. The `gemini-1.5-flash` model has been selected here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1c9cd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagine you have a really smart puppy.  You teach it tricks, like \"sit\" and \"fetch.\"  The more you teach it, the better it gets at those tricks.\n",
      "\n",
      "AI is kind of like that super smart puppy, but instead of learning tricks, it learns from information.  We give it lots and lots of information – like pictures of cats and dogs, or sentences in different languages – and it learns to recognize patterns.\n",
      "\n",
      "So, if you show the AI a new picture of a cat, it might say \"cat!\" because it learned what cats look like from all the pictures it saw before.  Or if you ask it to translate \"hello\" into Spanish, it might say \"hola\" because it learned that from all the sentences it was shown.\n",
      "\n",
      "The AI isn't actually *thinking* like you and me, it's just really good at finding patterns and following instructions based on the information it's been given. It's like a super fast, super smart parrot that can do amazing things with information!  It can even learn to play games, write stories, or even help doctors make diagnoses, but it still needs us to teach it and help it along the way.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Gemini model (gemini-1.5-flash)\n",
    "flash = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "# Generate content with the model\n",
    "response = flash.generate_content(\"Explain AI to me like I'm a kid.\")\n",
    "\n",
    "# Print the generated response\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62037bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Imagine you have a really smart puppy.  You teach it tricks, like \"sit\" and \"fetch.\"  The more you teach it, the better it gets at those tricks.\n",
       "\n",
       "AI is kind of like that super smart puppy, but instead of learning tricks, it learns from information.  We give it lots and lots of information – like pictures of cats and dogs, or sentences in different languages – and it learns to recognize patterns.\n",
       "\n",
       "So, if you show the AI a new picture of a cat, it might say \"cat!\" because it learned what cats look like from all the pictures it saw before.  Or if you ask it to translate \"hello\" into Spanish, it might say \"hola\" because it learned that from all the sentences it was shown.\n",
       "\n",
       "The AI isn't actually *thinking* like you and me, it's just really good at finding patterns and following instructions based on the information it's been given. It's like a super fast, super smart parrot that can do amazing things with information!  It can even learn to play games, write stories, or even help doctors make diagnoses, but it still needs us to teach it and help it along the way.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the results in Markdown format\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb202c2e",
   "metadata": {},
   "source": [
    "<span style=\"font-size:18px; color:rgba(0, 0, 0, 0.5);\">Run your first prompt</span>\n",
    "\n",
    "---\n",
    "The previous example uses a single-turn, text-in/text-out structure, but you can also set up a multi-turn chat structure too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb038e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello tsummey! It's nice to meet you.  How can I help you today?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat = flash.start_chat(history=[])\n",
    "response = chat.send_message('Hello! My name is tsummey.')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eacf94fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One interesting fact about dinosaurs is that some of them, like the *Therizinosaurus*, had incredibly long claws, some reaching up to three feet long!  Scientists aren't entirely sure what these claws were primarily used for, but theories range from defense against predators to reaching high into trees for food, or even for intraspecies competition (fighting amongst themselves).  The sheer size and unusual nature of these claws makes *Therizinosaurus* a truly fascinating and somewhat mysterious dinosaur.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message('Can you tell something interesting about dinosaurs?')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f19dcc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, your name is tsummey.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# While you have the `chat` object around, the conversation state\n",
    "# persists. Confirm that by asking if it knows my name.\n",
    "response = chat.send_message('Do you remember what my name is?')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a25338a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"The sky is blue due to a phenomenon called **Rayleigh scattering**.  Sunlight is made up of all the colors of the rainbow.  As sunlight enters the Earth's atmosphere, it collides with tiny air molecules (mostly nitrogen and oxygen).  These molecules are much smaller than the wavelengths of visible light.\\n\\nRayleigh scattering affects shorter wavelengths of light more strongly than longer wavelengths.  Blue and violet light have the shortest wavelengths, so they are scattered more effectively than other colors.  This scattered blue light is what we see when we look at the sky.\\n\\nViolet light actually has an even shorter wavelength than blue, and should therefore scatter even more. However, our eyes are less sensitive to violet, and the sun emits slightly less violet light, which is why we see a blue sky rather than a violet one.\\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"avg_logprobs\": -0.08686733816912075\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 168,\n",
       "        \"candidates_token_count\": 167,\n",
       "        \"total_token_count\": 335\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What does the raw response JSON look like\n",
    "chat.send_message('Why is the sky blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55ad9d3",
   "metadata": {},
   "source": [
    "<span style=\"font-size:18px; color:rgba(0, 0, 0, 0.5);\">Choose a model</span>\n",
    "\n",
    "---\n",
    "The Gemini API provides access to a number of models from the Gemini model family. Read about the available models and their capabilities on the [model overview page](https://ai.google.dev/gemini-api/docs/models/gemini). In this step you'll use the API to list all of the available models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f62b120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/chat-bison-001\n",
      "models/text-bison-001\n",
      "models/embedding-gecko-001\n",
      "models/gemini-1.0-pro-latest\n",
      "models/gemini-1.0-pro\n",
      "models/gemini-pro\n",
      "models/gemini-1.0-pro-001\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-pro-exp-0801\n",
      "models/gemini-1.5-pro-exp-0827\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-exp-0827\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-1.5-flash-8b-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0924\n",
      "models/learnlm-1.5-pro-experimental\n",
      "models/gemini-exp-1114\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/aqa\n"
     ]
    }
   ],
   "source": [
    "for model in genai.list_models():\n",
    "  print(model.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54cbbf49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(name='models/gemini-1.5-flash',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash',\n",
      "      description=('Alias that points to the most recent stable version of Gemini 1.5 Flash, our '\n",
      "                   'fast and versatile multimodal model for scaling across diverse tasks.'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n"
     ]
    }
   ],
   "source": [
    "for model in genai.list_models():\n",
    "  if model.name == 'models/gemini-1.5-flash':\n",
    "    print(model)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176beef1",
   "metadata": {},
   "source": [
    "<span style=\"font-size:18px; color:rgba(0, 0, 0, 0.5);\">Explore generation parameters</span>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5c5b2a",
   "metadata": {},
   "source": [
    "<span style=\"font-size:16px; color:rgba(0, 0, 0, 0.5);\">Output Length</span><br>\n",
    "When generating text with an LLM, the output length affects cost and performance. Generating more tokens increases computation, leading to higher energy consumption, latency, and cost.\n",
    "\n",
    "To stop the model from generating tokens past a limit, you can specify the `max_output_tokens` parameter when using the Gemini API. Specifying this parameter does not influence the generation of the output tokens, so the output will not become more stylistically or textually succinct, but it will stop generating tokens once the specified length is reached. Prompt engineering may be required to generate a more complete output for your given limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc224019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## The Enduring Significance of Olives in Modern Society\n",
      "\n",
      "The olive, a seemingly simple fruit, holds a profound and multifaceted importance in modern society, extending far beyond its culinary applications.  From its contribution to the global economy to its role in cultural heritage and even its potential in health and sustainability, the olive's impact resonates across various spheres of human life. Understanding this significance requires examining its economic influence, its deeply entrenched cultural identity, its health benefits, and finally, the challenges and opportunities related to its sustainable cultivation.\n",
      "\n",
      "Economically, the olive and its derivatives represent a significant industry worldwide. The Mediterranean basin, historically the heartland of olive cultivation, remains a major producer, with countries like Spain, Italy, Greece, and Turkey contributing substantially to global olive oil production.  This industry provides livelihoods for millions, encompassing farmers, processors, distributors, and retailers.  Olive oil, the primary product derived from olives, is a multi-billion dollar industry, driving economic growth in rural communities and contributing\n"
     ]
    }
   ],
   "source": [
    "short_model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash',\n",
    "    generation_config=genai.GenerationConfig(max_output_tokens=200))\n",
    "\n",
    "response = short_model.generate_content('Write a 1000 word essay on the importance of olives in modern society.')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fa705da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From ancient groves, a bounty small,\n",
      "The olive's grace, it conquers all.\n",
      "In oils so rich, a flavour deep,\n",
      "A Mediterranean promise to keep.\n",
      "\n",
      "On tables spread, a simple fare,\n",
      "In beauty products, beyond compare.\n",
      "From lotions smooth to soaps so mild,\n",
      "The olive's touch, both pure and wild.\n",
      "\n",
      "A symbol strong, of sun-drenched lands,\n",
      "In modern times, it still commands.\n",
      "A taste of history, vibrant, bright,\n",
      "The olive shines, a welcome light. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = short_model.generate_content('Write a short poem on the importance of olives in modern society.')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8dc9049c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## The Enduring Importance of Olives in Modern Society\n",
      "\n",
      "The olive (Olea europaea), a seemingly unassuming fruit, holds a position of profound importance in modern society that extends far beyond its culinary applications.  Its significance is woven into the fabric\n"
     ]
    }
   ],
   "source": [
    "# Changing the max_output_tokens to 50 and review the response change\n",
    "short_model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash',\n",
    "    generation_config=genai.GenerationConfig(max_output_tokens=50))\n",
    "\n",
    "response = short_model.generate_content('Write a 1000 word essay on the importance of olives in modern society.')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53e2726b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model returned 55 tokens\n",
      "\n",
      "1: ##\n",
      "2: ĠThe\n",
      "3: ĠEnd\n",
      "4: uring\n",
      "5: ĠImport\n",
      "6: ance\n",
      "7: Ġof\n",
      "8: ĠOl\n",
      "9: ives\n",
      "10: Ġin\n",
      "11: ĠModern\n",
      "12: ĠSociety\n",
      "13: Ċ\n",
      "14: Ċ\n",
      "15: The\n",
      "16: Ġolive\n",
      "17: Ġ(\n",
      "18: O\n",
      "19: le\n",
      "20: a\n",
      "21: Ġeuro\n",
      "22: p\n",
      "23: aea\n",
      "24: ),\n",
      "25: Ġa\n",
      "26: Ġseemingly\n",
      "27: Ġun\n",
      "28: assuming\n",
      "29: Ġfruit\n",
      "30: ,\n",
      "31: Ġholds\n",
      "32: Ġa\n",
      "33: Ġposition\n",
      "34: Ġof\n",
      "35: Ġprofound\n",
      "36: Ġimportance\n",
      "37: Ġin\n",
      "38: Ġmodern\n",
      "39: Ġsociety\n",
      "40: Ġthat\n",
      "41: Ġextends\n",
      "42: Ġfar\n",
      "43: Ġbeyond\n",
      "44: Ġits\n",
      "45: Ġculinary\n",
      "46: Ġapplications\n",
      "47: .\n",
      "48: Ġ\n",
      "49: ĠIts\n",
      "50: Ġsignificance\n",
      "51: Ġis\n",
      "52: Ġwoven\n",
      "53: Ġinto\n",
      "54: Ġthe\n",
      "55: Ġfabric\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer (adjust the model name as per your use case)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Tokenize the response text\n",
    "tokens = tokenizer.tokenize(response.text)\n",
    "\n",
    "# Token check\n",
    "print('The model returned',len(tokens),'tokens\\n')  # Should return 50 for strict compliance\n",
    "\n",
    "# Print each token with its index,Ġ represents a space and Ċ a new line.\n",
    "for idx, token in enumerate(tokens, start=1):  # start=1 ensures the count starts at 1\n",
    "    print(f\"{idx}: {token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc04e12d",
   "metadata": {},
   "source": [
    "<span style=\"font-size:18px; color:rgba(0, 0, 0, 0.5);\">Why you see more tokens that expected</span>\n",
    "\n",
    "---\n",
    "Finalizing the Response: The tokenizer added an extra tokens to ensure the response ends coherently. For example:\n",
    "\n",
    "If token 50 was \"A\", the model might have included \" \" (space) or the next character, resulting in token 51.\n",
    "Formatting Artifacts: Special tokens (like line breaks or spaces) could sneak in during tokenization, inflating the count.\n",
    "\n",
    "Implementation Quirk: The specific library you're using might misinterpret the token limit during post-processing, including the last token even if it slightly exceeds the set maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81459268",
   "metadata": {},
   "source": [
    "<span style=\"font-size:18px; color:rgba(0, 0, 0, 0.5);\">Display Tokens and IDs</span>\n",
    "\n",
    "---\n",
    "Tokenization is the bridge between raw text and numerical computations. The model learns to associate tokens with context, enabling it to predict and generate coherent responses in the correct order. This process is the heart of training and inference in language models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "660ed78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: Token: ##, ID: 2235\n",
      "2: Token: ĠThe, ID: 383\n",
      "3: Token: ĠEnd, ID: 5268\n",
      "4: Token: uring, ID: 870\n",
      "5: Token: ĠImport, ID: 17267\n",
      "6: Token: ance, ID: 590\n",
      "7: Token: Ġof, ID: 286\n",
      "8: Token: ĠOl, ID: 6544\n",
      "9: Token: ives, ID: 1083\n",
      "10: Token: Ġin, ID: 287\n",
      "11: Token: ĠModern, ID: 12495\n",
      "12: Token: ĠSociety, ID: 7023\n",
      "13: Token: Ċ, ID: 198\n",
      "14: Token: Ċ, ID: 198\n",
      "15: Token: The, ID: 464\n",
      "16: Token: Ġolive, ID: 19450\n",
      "17: Token: Ġ(, ID: 357\n",
      "18: Token: O, ID: 46\n",
      "19: Token: le, ID: 293\n",
      "20: Token: a, ID: 64\n",
      "21: Token: Ġeuro, ID: 11063\n",
      "22: Token: p, ID: 79\n",
      "23: Token: aea, ID: 44705\n",
      "24: Token: ),, ID: 828\n",
      "25: Token: Ġa, ID: 257\n",
      "26: Token: Ġseemingly, ID: 9775\n",
      "27: Token: Ġun, ID: 555\n",
      "28: Token: assuming, ID: 32935\n",
      "29: Token: Ġfruit, ID: 8234\n",
      "30: Token: ,, ID: 11\n",
      "31: Token: Ġholds, ID: 6622\n",
      "32: Token: Ġa, ID: 257\n",
      "33: Token: Ġposition, ID: 2292\n",
      "34: Token: Ġof, ID: 286\n",
      "35: Token: Ġprofound, ID: 11982\n",
      "36: Token: Ġimportance, ID: 6817\n",
      "37: Token: Ġin, ID: 287\n",
      "38: Token: Ġmodern, ID: 3660\n",
      "39: Token: Ġsociety, ID: 3592\n",
      "40: Token: Ġthat, ID: 326\n",
      "41: Token: Ġextends, ID: 14582\n",
      "42: Token: Ġfar, ID: 1290\n",
      "43: Token: Ġbeyond, ID: 3675\n",
      "44: Token: Ġits, ID: 663\n",
      "45: Token: Ġculinary, ID: 35956\n",
      "46: Token: Ġapplications, ID: 5479\n",
      "47: Token: ., ID: 13\n",
      "48: Token: Ġ, ID: 220\n",
      "49: Token: ĠIts, ID: 6363\n",
      "50: Token: Ġsignificance, ID: 12085\n",
      "51: Token: Ġis, ID: 318\n",
      "52: Token: Ġwoven, ID: 36932\n",
      "53: Token: Ġinto, ID: 656\n",
      "54: Token: Ġthe, ID: 262\n",
      "55: Token: Ġfabric, ID: 9664\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Tokenize the response text and get the token IDs\n",
    "tokens = tokenizer.tokenize(response.text)\n",
    "token_ids = tokenizer.encode(response.text)\n",
    "\n",
    "# Print each token with its corresponding integer ID\n",
    "for idx, (token, token_id) in enumerate(zip(tokens, token_ids), start=1):\n",
    "    print(f\"{idx}: Token: {token}, ID: {token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf58037",
   "metadata": {},
   "source": [
    "<span style=\"font-size:18px; color:rgba(0, 0, 0, 0.5);\">Temperature</span>\n",
    "\n",
    "---\n",
    "Temperature controls the degree of randomness in token selection. Higher temperatures result in a higher number of candidate tokens from which the next output token is selected, and can produce more diverse results, while lower temperatures have the opposite effect, such that a temperature of 0 results in greedy decoding, selecting the most probable token at each step.\n",
    "\n",
    "Temperature doesn't provide any guarantees of randomness, but it can be used to \"nudge\" the output somewhat.\n",
    "\n",
    "<font color=red>Note: If you see a 429 Resource Exhausted error here, you may be able to edit the words in the prompt slightly to progress.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0761b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maroon\n",
      " -------------------------\n",
      "Maroon\n",
      " -------------------------\n",
      "Maroon\n",
      " -------------------------\n",
      "Aquamarine\n",
      " -------------------------\n",
      "Maroon\n",
      " -------------------------\n"
     ]
    }
   ],
   "source": [
    "high_temp_model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash',\n",
    "    generation_config=genai.GenerationConfig(temperature=2.0))\n",
    "\n",
    "\n",
    "# When running lots of queries, it's a good practice to use a retry policy so your code\n",
    "# automatically retries when hitting Resource Exhausted (quota limit) errors.\n",
    "retry_policy = {\n",
    "    \"retry\": retry.Retry(predicate=retry.if_transient_error, initial=10, multiplier=1.5, timeout=300)\n",
    "}\n",
    "\n",
    "for _ in range(5):\n",
    "  response = high_temp_model.generate_content('Pick a random colour... (respond in a single word)',\n",
    "                                              request_options=retry_policy)\n",
    "  if response.parts:\n",
    "    print(response.text, '-' * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2472d205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marigold\n",
      " -------------------------\n",
      "Maroon\n",
      " -------------------------\n",
      "Purple\n",
      " -------------------------\n",
      "Aquamarine\n",
      " -------------------------\n",
      "Purple\n",
      " -------------------------\n"
     ]
    }
   ],
   "source": [
    "high_temp_model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash',\n",
    "    generation_config=genai.GenerationConfig(\n",
    "        temperature=2.0\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# When running lots of queries, it's a good practice to use a retry policy so your code\n",
    "# automatically retries when hitting Resource Exhausted (quota limit) errors.\n",
    "retry_policy = {\n",
    "    \"retry\": retry.Retry(predicate=retry.if_transient_error, initial=10, multiplier=1.5, timeout=300)\n",
    "}\n",
    "\n",
    "for _ in range(5):\n",
    "  response = high_temp_model.generate_content('Pick a random colour... (respond in a single word)',\n",
    "                                              request_options=retry_policy)\n",
    "  if response.parts:\n",
    "    print(response.text, '-' * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4fab8056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maroon\n",
      " -------------------------\n",
      "Maroon\n",
      " -------------------------\n",
      "Maroon\n",
      " -------------------------\n",
      "Maroon\n",
      " -------------------------\n",
      "Maroon\n",
      " -------------------------\n"
     ]
    }
   ],
   "source": [
    "low_temp_model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash',\n",
    "    generation_config=genai.GenerationConfig(temperature=0.0))\n",
    "\n",
    "for _ in range(5):\n",
    "  response = low_temp_model.generate_content('Pick a random colour... (respond in a single word)',\n",
    "                                             request_options=retry_policy)\n",
    "  if response.parts:\n",
    "    print(response.text, '-' * 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53985239",
   "metadata": {},
   "source": [
    "<span style=\"font-size:18px; color:rgba(0, 0, 0, 0.5);\">Top-K and Top-P</span>\n",
    "\n",
    "---\n",
    "Like temperature, top-K and top-P parameters are also used to control the diversity of the model's output.\n",
    "\n",
    "Top-K is a positive integer that defines the number of most probable tokens from which to select the output token. A top-K of 1 selects a single token, performing greedy decoding.\n",
    "\n",
    "Top-P defines the probability threshold that, once cumulatively exceeded, tokens stop being selected as candidates. A top-P of 0 is typically equivalent to greedy decoding, and a top-P of 1 typically selects every token in the model's vocabulary.\n",
    "\n",
    "When both are supplied, the Gemini API will filter top-K tokens first, then top-P and then finally sample from the candidate tokens using the supplied temperature.\n",
    "\n",
    "Run this example a number of times, change the settings and observe the change in output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b33eb968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bartholomew, a sleek black cat with eyes like polished emeralds, was bored. His days were a monotonous cycle of sunbeams, naps, and the occasional mouse chase. One morning, while perched on the windowsill, he saw a flash of brilliant blue – a robin, perched on the fencepost, chirping a cheerful tune. \n",
      "\n",
      "Something stirred within Bartholomew. He yearned for adventure, for something more exciting than the predictable routine of his life. He watched the robin, its tiny chest puffed with song, and a plan began to form. He would follow the robin, see where it led him. \n",
      "\n",
      "Bartholomew slipped out the open window, the robin's song his guiding melody. He scampered through the garden, a blur of black fur against the vibrant green. He chased the robin through the meadow, across a babbling brook, and into the heart of a whispering forest.\n",
      "\n",
      "The forest was a world unlike any Bartholomew had known. Tall trees cast long, dappled shadows on the mossy ground. Strange smells tickled his nose, and whispers rustled through the leaves. He followed the robin deeper into the woods, his emerald eyes wide with wonder.\n",
      "\n",
      "Suddenly, the robin disappeared. Bartholomew stopped, his ears twitching, listening. He heard a soft mewling coming from a thicket of bushes. He cautiously made his way through the undergrowth and found a tiny, lost kitten. \n",
      "\n",
      "The kitten, no bigger than his paw, was huddled under a bush, shivering with fear. Bartholomew, despite his own adventurous spirit, knew the dangers of the forest. He gently nudged the kitten with his nose, urging it to follow him. \n",
      "\n",
      "Together, they explored the forest, the older cat guiding the younger, sharing the spoils of their hunts: a juicy grasshopper, a plump beetle, a juicy worm. They slept curled together for warmth, the whispering leaves their lullaby. \n",
      "\n",
      "Days turned into weeks, and Bartholomew, the bored house cat, had become a seasoned adventurer. He taught the kitten how to stalk, how to climb, how to survive in the wild. He even learned to appreciate the silence of the forest, the beauty of the moonlit sky, and the thrill of the hunt.\n",
      "\n",
      "One day, the kitten, now strong and playful, looked at Bartholomew with a mischievous glint in his eyes. \"Come on, old man,\" he meowed, \"let's go home.\"\n",
      "\n",
      "Bartholomew looked at the kitten, then at the familiar trail leading back to his home. He had missed the warmth of the sunbeam on his fur, the scent of his human's perfume, the comfort of his cozy bed. He smiled, a silent promise of adventure in his emerald eyes. \n",
      "\n",
      "They walked back together, the kitten bounding ahead, Bartholomew following with a slower, but steady pace. He had found adventure, and he had found a friend. He knew he would always be a house cat at heart, but he was no longer just a cat; he was Bartholomew, the adventurer, and that was a story he would carry with him forever. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-001',\n",
    "    generation_config=genai.GenerationConfig(\n",
    "        # These are the default values for gemini-1.5-flash-001.\n",
    "        temperature=1.0,\n",
    "        top_k=64,\n",
    "        top_p=0.95,\n",
    "    ))\n",
    "\n",
    "story_prompt = \"You are a creative writer. Write a short story about a cat who goes on an adventure.\"\n",
    "response = model.generate_content(story_prompt, request_options=retry_policy)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6033526b",
   "metadata": {},
   "source": [
    "<span style=\"font-size:18px; color:rgba(0, 0, 0, 0.5);\">Prompting</span>\n",
    "\n",
    "---\n",
    "\n",
    "This section contains some prompts from the chapter for you to try out directly in the API. Try changing the text here to see how each prompt performs with different instructions, more examples, or any other changes you can think of."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447a5812",
   "metadata": {},
   "source": [
    "<span style=\"font-size:16px; color:rgba(0, 0, 0, 0.5);\">Zero-shot</span>\n",
    "\n",
    "Zero-shot prompts are prompts that describe the request for the model directly.\n",
    "\n",
    "<table align=left>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://aistudio.google.com/prompts/1gzKKgDHwkAvexG5Up0LMtl1-6jKMKe4g\"><img src=\"https://ai.google.dev/site-assets/images/marketing/home/icon-ais.png\" style=\"height: 24px\" height=24/> Open in AI Studio</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eadf987a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: **POSITIVE**\n"
     ]
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-001',\n",
    "    generation_config=genai.GenerationConfig(\n",
    "        temperature=0.1,\n",
    "        top_p=1,\n",
    "        max_output_tokens=5,\n",
    "    ))\n",
    "\n",
    "zero_shot_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\n",
    "Review: \"Her\" is a disturbing study revealing the direction\n",
    "humanity is headed if AI is allowed to keep evolving,\n",
    "unchecked. I wish there were more movies like this masterpiece.\n",
    "Sentiment: \"\"\"\n",
    "\n",
    "response = model.generate_content(zero_shot_prompt, request_options=retry_policy)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc3f891",
   "metadata": {},
   "source": [
    "<span style=\"font-size:16px; color:rgba(0, 0, 0, 0.5);\">Enum mode</span>\n",
    "\n",
    "The models are trained to generate text, and can sometimes produce more text than you may wish for. In the preceding example, the model will output the label, sometimes it can include a preceding \"Sentiment\" label, and without an output token limit, it may also add explanatory text afterwards.\n",
    "\n",
    "The Gemini API has an [Enum mode](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Enum.ipynb) feature that allows you to constrain the output to a fixed set of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "948ddfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The sentiment of the statement \"I love this product!\" is **extremely positive**. \n",
      "\n",
      "The words \"love\" and \"product\" directly indicate a strong positive feeling towards the item being discussed. \n",
      "\n",
      "Sentiment: Sentiment.POSITIVE\n"
     ]
    }
   ],
   "source": [
    "# This code below was modified from the original Kaggle notebook due to the response\n",
    "# AttributeError: type object 'dummy' has no attribute 'model_json_schema'\n",
    "\n",
    "class Sentiment(enum.Enum):\n",
    "    POSITIVE = \"positive\"\n",
    "    NEUTRAL = \"neutral\"\n",
    "    NEGATIVE = \"negative\"\n",
    "\n",
    "# Configure and initialize the model without unsupported attributes\n",
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-001',\n",
    "    generation_config=genai.GenerationConfig()\n",
    ")\n",
    "\n",
    "# Provide a prompt and handle response interpretation manually\n",
    "zero_shot_prompt = \"What is the sentiment of this statement: 'I love this product!'\"\n",
    "\n",
    "response = model.generate_content(zero_shot_prompt)\n",
    "text_response = response.text\n",
    "print(\"Response:\", text_response)\n",
    "\n",
    "# Simple interpretation (manual sentiment analysis)\n",
    "if \"positive\" in text_response.lower():\n",
    "    sentiment = Sentiment.POSITIVE\n",
    "elif \"neutral\" in text_response.lower():\n",
    "    sentiment = Sentiment.NEUTRAL\n",
    "elif \"negative\" in text_response.lower():\n",
    "    sentiment = Sentiment.NEGATIVE\n",
    "else:\n",
    "    sentiment = \"Unknown\"\n",
    "\n",
    "print(\"Sentiment:\", sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691181aa",
   "metadata": {},
   "source": [
    "<span style=\"font-size:16px; color:rgba(0, 0, 0, 0.5);\">One-shot and Few-shot</span>\n",
    "\n",
    "Providing an example of the expected response is known as a \"one-shot\" prompt. When you provide multiple examples, it is a \"few-shot\" prompt.\n",
    "\n",
    "<table align=left>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://aistudio.google.com/prompts/1jjWkjUSoMXmLvMJ7IzADr_GxHPJVV2bg\"><img src=\"https://ai.google.dev/site-assets/images/marketing/home/icon-ais.png\" style=\"height: 24px\" height=24/> Open in AI Studio</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32cd0b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"size\": \"large\",\n",
      "  \"type\": \"normal\",\n",
      "  \"ingredients\": [\"cheese\", \"pineapple\"]\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-latest',\n",
    "    generation_config=genai.GenerationConfig(\n",
    "        temperature=0.1,\n",
    "        top_p=1,\n",
    "        max_output_tokens=250,\n",
    "    ))\n",
    "\n",
    "few_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n",
    "\n",
    "EXAMPLE:\n",
    "I want a small pizza with cheese, tomato sauce, and pepperoni.\n",
    "JSON Response:\n",
    "```\n",
    "{\n",
    "\"size\": \"small\",\n",
    "\"type\": \"normal\",\n",
    "\"ingredients\": [\"cheese\", \"tomato sauce\", \"peperoni\"]\n",
    "}\n",
    "```\n",
    "\n",
    "EXAMPLE:\n",
    "Can I get a large pizza with tomato sauce, basil and mozzarella\n",
    "JSON Response:\n",
    "```\n",
    "{\n",
    "\"size\": \"large\",\n",
    "\"type\": \"normal\",\n",
    "\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n",
    "}\n",
    "\n",
    "ORDER:\n",
    "\"\"\"\n",
    "\n",
    "customer_order = \"Give me a large with cheese & pineapple\"\n",
    "\n",
    "\n",
    "response = model.generate_content([few_shot_prompt, customer_order])\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ace3de5",
   "metadata": {},
   "source": [
    "<span style=\"font-size:16px; color:rgba(0, 0, 0, 0.5);\">JSON mode</span>\n",
    "\n",
    "To provide control over the schema, and to ensure that you only receive JSON (with no other text or markdown), you can use the Gemini API's [JSON mode](https://github.com/google-gemini/cookbook/blob/main/quickstarts/JSON_mode.ipynb). This forces the model to constrain decoding, such that token selection is guided by the supplied schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ec6d378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw response: ```json\n",
      "{\n",
      "  \"size\": \"large\",\n",
      "  \"ingredients\": [\"apple\", \"chocolate\"],\n",
      "  \"type\": \"dessert pizza\"\n",
      "}\n",
      "```\n",
      "\n",
      "Parsed Pizza Order: {'size': 'large', 'ingredients': ['apple', 'chocolate'], 'type': 'dessert pizza'}\n"
     ]
    }
   ],
   "source": [
    "# The code below was modified, the markdown formatting would result in\n",
    "# Error: The response was not valid JSON. The code below works.\n",
    "\n",
    "# Define the TypedDict for expected JSON structure\n",
    "class PizzaOrder(TypedDict):\n",
    "    size: str\n",
    "    ingredients: list[str]\n",
    "    type: str\n",
    "\n",
    "# Configure the model with appropriate generation settings\n",
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-latest',\n",
    "    generation_config=genai.GenerationConfig(\n",
    "        temperature=0.1\n",
    "    )\n",
    ")\n",
    "\n",
    "# Prompt instructing the model to return a pure JSON response\n",
    "prompt = \"\"\"\n",
    "Can I have a large dessert pizza with apple and chocolate?\n",
    "Please respond in pure JSON format without any extra formatting or explanations:\n",
    "{\n",
    "    \"size\": \"string\",\n",
    "    \"ingredients\": [\"string\"],\n",
    "    \"type\": \"string\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Generate the response\n",
    "response = model.generate_content(prompt)\n",
    "response_text = response.text\n",
    "\n",
    "# Print raw response for debugging\n",
    "print(\"Raw response:\", response_text)\n",
    "\n",
    "# Remove any Markdown formatting\n",
    "cleaned_response = response_text.strip(\"```\").replace(\"json\\n\", \"\").replace(\"\\n```\", \"\")\n",
    "\n",
    "# Try parsing the cleaned response as JSON\n",
    "try:\n",
    "    pizza_order: PizzaOrder = json.loads(cleaned_response)\n",
    "    print(\"Parsed Pizza Order:\", pizza_order)\n",
    "except json.JSONDecodeError as e:\n",
    "    print(\"Error: The response was not valid JSON.\")\n",
    "    print(\"Details:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a301389a",
   "metadata": {},
   "source": [
    "<span style=\"font-size:16px; color:rgba(0, 0, 0, 0.5);\">Chain of Thought (CoT)</span>\n",
    "\n",
    "Direct prompting on LLMs can return answers quickly and (in terms of output token usage) efficiently, but they can be prone to hallucination. The answer may \"look\" correct (in terms of language and syntax) but is incorrect in terms of factuality and reasoning.\n",
    "\n",
    "Chain-of-Thought prompting is a technique where you instruct the model to output intermediate reasoning steps, and it typically gets better results, especially when combined with few-shot examples. It is worth noting that this technique doesn't completely eliminate hallucinations, and that it tends to cost more to run, due to the increased token count.\n",
    "\n",
    "As models like the Gemini family are trained to be \"chatty\" and provide reasoning steps, you can ask the model to be more direct in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d7976cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"When I was 4 years old, my wife was 3 times my age. Now, I\n",
    "am 20 years old. How old is my wife? Return the answer directly.\"\"\"\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "response = model.generate_content(prompt, request_options=retry_policy)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67a7b5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's how to solve this step-by-step:\n",
      "\n",
      "1. **Wife's age when you were 4:** When you were 4, your wife was 3 times your age, meaning she was 4 * 3 = 12 years old.\n",
      "\n",
      "2. **Age difference:** The age difference between you and your wife is 12 - 4 = 8 years.\n",
      "\n",
      "3. **Wife's current age:**  Since you are now 20, your wife is 20 + 8 = 28 years old.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"When I was 4 years old, my wife was 3 times my age. Now,\n",
    "I am 20 years old. How old is my wife? Let's think step by step.\"\"\"\n",
    "\n",
    "response = model.generate_content(prompt, request_options=retry_policy)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993109f5",
   "metadata": {},
   "source": [
    "<span style=\"font-size:16px; color:rgba(0, 0, 0, 0.5);\">ReAct: Reason and act</span>\n",
    "\n",
    "In this example you will run a ReAct prompt directly in the Gemini API and perform the searching steps yourself. As this prompt follows a well-defined structure, there are frameworks available that wrap the prompt into easier-to-use APIs that make tool calls automatically, such as the LangChain example from the chapter.\n",
    "\n",
    "To try this out with the Wikipedia search engine, check out the [Searching Wikipedia with ReAct](https://github.com/google-gemini/cookbook/blob/main/examples/Search_Wikipedia_using_ReAct.ipynb) cookbook example.\n",
    "\n",
    "\n",
    "> Note: The prompt and in-context examples used here are from [https://github.com/ysymyth/ReAct](https://github.com/ysymyth/ReAct) which is published under a [MIT license](https://opensource.org/licenses/MIT), Copyright (c) 2023 Shunyu Yao.\n",
    "\n",
    "<table align=left>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://aistudio.google.com/prompts/18oo63Lwosd-bQ6Ay51uGogB3Wk3H8XMO\"><img src=\"https://ai.google.dev/site-assets/images/marketing/home/icon-ais.png\" style=\"height: 24px\" height=24/> Open in AI Studio</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47841410",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_instructions = \"\"\"\n",
    "Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\n",
    "Observation is understanding relevant information from an Action's output and Action can be one of three types:\n",
    " (1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n",
    "     will return some similar entities to search and you can try to search the information from those topics.\n",
    " (2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches,\n",
    "     so keep your searches short.\n",
    " (3) <finish>answer</finish>, which returns the answer and finishes the task.\n",
    "\"\"\"\n",
    "\n",
    "example1 = \"\"\"Question\n",
    "Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n",
    "\n",
    "Thought 1\n",
    "The question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n",
    "\n",
    "Action 1\n",
    "<search>Milhouse</search>\n",
    "\n",
    "Observation 1\n",
    "Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n",
    "\n",
    "Thought 2\n",
    "The paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n",
    "\n",
    "Action 2\n",
    "<lookup>named after</lookup>\n",
    "\n",
    "Observation 2\n",
    "Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n",
    "\n",
    "Thought 3\n",
    "Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n",
    "\n",
    "Action 3\n",
    "<finish>Richard Nixon</finish>\n",
    "\"\"\"\n",
    "\n",
    "example2 = \"\"\"Question\n",
    "What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n",
    "\n",
    "Thought 1\n",
    "I need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n",
    "\n",
    "Action 1\n",
    "<search>Colorado orogeny</search>\n",
    "\n",
    "Observation 1\n",
    "The Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n",
    "\n",
    "Thought 2\n",
    "It does not mention the eastern sector. So I need to look up eastern sector.\n",
    "\n",
    "Action 2\n",
    "<lookup>eastern sector</lookup>\n",
    "\n",
    "Observation 2\n",
    "The eastern sector extends into the High Plains and is called the Central Plains orogeny.\n",
    "\n",
    "Thought 3\n",
    "The eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n",
    "\n",
    "Action 3\n",
    "<search>High Plains</search>\n",
    "\n",
    "Observation 3\n",
    "High Plains refers to one of two distinct land regions\n",
    "\n",
    "Thought 4\n",
    "I need to instead search High Plains (United States).\n",
    "\n",
    "Action 4\n",
    "<search>High Plains (United States)</search>\n",
    "\n",
    "Observation 4\n",
    "The High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130m).\n",
    "\n",
    "Thought 5\n",
    "High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n",
    "\n",
    "Action 5\n",
    "<finish>1,800 to 7,000 ft</finish>\n",
    "\"\"\"\n",
    "\n",
    "example3 = \"\"\"Question\n",
    "What is the scientific classification (genus and species) of the animal commonly known as the giant panda?\n",
    "\n",
    "Thought 1:\n",
    "To answer this question, I need to search for \"giant panda\" to find its scientific classification.\n",
    "\n",
    "Action 1:\n",
    "<search>giant panda</search>\n",
    "\n",
    "Observation 1:\n",
    "The giant panda (Ailuropoda melanoleuca) is a bear species native to China, characterized by its bold black-and-white coat.\n",
    "\n",
    "Thought 2:\n",
    "The observation contains the scientific classification for the giant panda. The genus is \"Ailuropoda\" and the species is \"melanoleuca,\" so the answer is Ailuropoda melanoleuca.\n",
    "\n",
    "Action 2:\n",
    "<finish>Ailuropoda melanoleuca</finish>\n",
    "\"\"\"\n",
    "\n",
    "# Come up with more examples yourself, or take a look through https://github.com/ysymyth/ReAct/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962eb679",
   "metadata": {},
   "source": [
    " To capture a single step at a time, while ignoring any hallucinated Observation steps, you will use stop_sequences to end the generation process. The steps are Thought, Action, Observation, in that order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3a273dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought 1\n",
      "I need to find the Transformers NLP paper and look for the authors' ages.  This will require searching for the paper and then likely some further searching to find the authors' ages.\n",
      "\n",
      "Action 1\n",
      "<search>Transformers NLP paper</search>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"Question\n",
    "Who was the youngest author listed on the transformers NLP paper?\n",
    "\"\"\"\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "react_chat = model.start_chat()\n",
    "\n",
    "# You will perform the Action, so generate up to, but not including, the Observation.\n",
    "config = genai.GenerationConfig(stop_sequences=[\"\\nObservation\"])\n",
    "\n",
    "resp = react_chat.send_message(\n",
    "    [model_instructions, example1, example2, question],\n",
    "    generation_config=config,\n",
    "    request_options=retry_policy)\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64f2d77",
   "metadata": {},
   "source": [
    "Now you can perform this research yourself and supply it back to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7959a5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought 2\n",
      "The observation provides the authors of the paper \"Attention is All You Need\".  I don't have their ages, so I'll need to search for each author individually to find their birthdates.  This is likely to be time-consuming and may not be possible for all authors.  I will focus on finding the youngest.\n",
      "\n",
      "Action 2\n",
      "<search>Aidan N. Gomez age>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "observation = \"\"\"Observation 1\n",
    "[1706.03762] Attention Is All You Need\n",
    "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\n",
    "We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n",
    "\"\"\"\n",
    "resp = react_chat.send_message(observation, generation_config=config)\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ff0813",
   "metadata": {},
   "source": [
    "This process repeats until the `<finish>` action is reached. You can continue running this yourself if you like, or try the [Wikipedia example](https://github.com/google-gemini/cookbook/blob/main/examples/Search_Wikipedia_using_ReAct.ipynb) to see a fully automated ReAct system at work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72c12d2",
   "metadata": {},
   "source": [
    "<span style=\"font-size:18px; color:rgba(0, 0, 0, 0.5);\">Code Prompting</span>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682e9db1",
   "metadata": {},
   "source": [
    "<span style=\"font-size:16px; color:rgba(0, 0, 0, 0.5);\">Generating Code</span>\n",
    "\n",
    "The Gemini family of models can be used to generate code, configuration and scripts. Generating code can be helpful when learning to code, learning a new language or for rapidly generating a first draft.\n",
    "\n",
    "It's important to be aware that since LLMs can't reason, and can repeat training data, it's essential to read and test your code first, and comply with any relevant licenses.\n",
    "\n",
    "<table align=left>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://aistudio.google.com/prompts/1YX71JGtzDjXQkgdes8bP6i3oH5lCRKxv\"><img src=\"https://ai.google.dev/site-assets/images/marketing/home/icon-ais.png\" style=\"height: 24px\" height=24/> Open in AI Studio</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3f26a76d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "def factorial(n):\n",
       "  if n == 0:\n",
       "    return 1\n",
       "  else:\n",
       "    return n * factorial(n-1)\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-latest',\n",
    "    generation_config=genai.GenerationConfig(\n",
    "        temperature=1,\n",
    "        top_p=1,\n",
    "        max_output_tokens=1024,\n",
    "    ))\n",
    "\n",
    "# Gemini 1.5 models are very chatty, so it helps to specify they stick to the code.\n",
    "code_prompt = \"\"\"\n",
    "Write a Python function to calculate the factorial of a number. No explanation, provide only the code.\n",
    "\"\"\"\n",
    "\n",
    "response = model.generate_content(code_prompt, request_options=retry_policy)\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f11ebf",
   "metadata": {},
   "source": [
    "<span style=\"font-size:16px; color:rgba(0, 0, 0, 0.5);\">Code execution</span>\n",
    "\n",
    "The Gemini API can automatically run generated code too, and will return the output.\n",
    "\n",
    "<table align=left>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://aistudio.google.com/prompts/11veFr_VYEwBWcLkhNLr-maCG0G8sS_7Z\"><img src=\"https://ai.google.dev/site-assets/images/marketing/home/icon-ais.png\" style=\"height: 24px\" height=24/> Open in AI Studio</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f6a5b2d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To calculate the sum of the first 14 odd prime numbers, I need to first identify those primes.  I will use Python to accomplish this.\n",
       "\n",
       "\n",
       "``` python\n",
       "import sympy\n",
       "\n",
       "primes = []\n",
       "count = 0\n",
       "num = 3 #Start from 3, the first odd prime\n",
       "\n",
       "while count < 14:\n",
       "    if sympy.isprime(num):\n",
       "        primes.append(num)\n",
       "        count += 1\n",
       "    num += 2 # Increment by 2 to only consider odd numbers\n",
       "\n",
       "print(f'{primes=}')\n",
       "sum_of_primes = sum(primes)\n",
       "print(f'{sum_of_primes=}')\n",
       "\n",
       "```\n",
       "```\n",
       "primes=[3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]\n",
       "sum_of_primes=326\n",
       "\n",
       "```\n",
       "The code first initializes an empty list `primes` to store the prime numbers and a counter `count` to track the number of primes found. It starts checking for prime numbers from 3, incrementing by 2 in each step to consider only odd numbers. The `sympy.isprime()` function efficiently determines if a number is prime. Once 14 odd prime numbers are found, the loop stops, and the sum of the numbers in the `primes` list is calculated and printed.\n",
       "\n",
       "Therefore, the sum of the first 14 odd prime numbers is 504.  The output shows a sum of 326.  There was an error in my initial reasoning.  My apologies.  The output from the code is correct.  The sum of the first 14 odd primes is 326.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-latest',\n",
    "    tools='code_execution',)\n",
    "\n",
    "code_exec_prompt = \"\"\"\n",
    "Calculate the sum of the first 14 prime numbers. Only consider the odd primes, and make sure you count them all.\n",
    "\"\"\"\n",
    "\n",
    "response = model.generate_content(code_exec_prompt, request_options=retry_policy)\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118b951a",
   "metadata": {},
   "source": [
    "While this looks like a single-part response, you can inspect the response to see the each of the steps: initial text, code generation, execution results, and final text summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef164b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: \"To calculate the sum of the first 14 odd prime numbers, I need to first identify those primes.  I will use Python to accomplish this.\\n\\n\"\n",
      "\n",
      "-----\n",
      "executable_code {\n",
      "  language: PYTHON\n",
      "  code: \"\\nimport sympy\\n\\nprimes = []\\ncount = 0\\nnum = 3 #Start from 3, the first odd prime\\n\\nwhile count < 14:\\n    if sympy.isprime(num):\\n        primes.append(num)\\n        count += 1\\n    num += 2 # Increment by 2 to only consider odd numbers\\n\\nprint(f\\'{primes=}\\')\\nsum_of_primes = sum(primes)\\nprint(f\\'{sum_of_primes=}\\')\\n\"\n",
      "}\n",
      "\n",
      "-----\n",
      "code_execution_result {\n",
      "  outcome: OUTCOME_OK\n",
      "  output: \"primes=[3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]\\nsum_of_primes=326\\n\"\n",
      "}\n",
      "\n",
      "-----\n",
      "text: \"The code first initializes an empty list `primes` to store the prime numbers and a counter `count` to track the number of primes found. It starts checking for prime numbers from 3, incrementing by 2 in each step to consider only odd numbers. The `sympy.isprime()` function efficiently determines if a number is prime. Once 14 odd prime numbers are found, the loop stops, and the sum of the numbers in the `primes` list is calculated and printed.\\n\\nTherefore, the sum of the first 14 odd prime numbers is 504.  The output shows a sum of 326.  There was an error in my initial reasoning.  My apologies.  The output from the code is correct.  The sum of the first 14 odd primes is 326.\\n\"\n",
      "\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "for part in response.candidates[0].content.parts:\n",
    "  print(part)\n",
    "  print(\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7a6ae8",
   "metadata": {},
   "source": [
    "<span style=\"font-size:16px; color:rgba(0, 0, 0, 0.5);\">Explaining Code</span>\n",
    "\n",
    "The Gemini family of models can explain code to you too.\n",
    "\n",
    "<table align=left>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://aistudio.google.com/prompts/1N7LGzWzCYieyOf_7bAG4plrmkpDNmUyb\"><img src=\"https://ai.google.dev/site-assets/images/marketing/home/icon-ais.png\" style=\"height: 24px\" height=24/> Open in AI Studio</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "04f2009a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This file is a bash script that provides a highly customizable Git prompt for your terminal.  In essence, it enhances your command-line interface to show concise information about your current Git repository, such as the branch, status (clean, modified files, etc.), and potentially even the remote URL and upstream tracking information.\n",
       "\n",
       "You would use it to:\n",
       "\n",
       "* **Improve your Git workflow:** By having Git status information directly in your prompt, you don't need to constantly run `git status` to check your repository's state.  This makes working with Git faster and more efficient.\n",
       "\n",
       "* **Customize your terminal appearance:**  The script allows you to extensively customize the colors and formatting of the prompt elements (branch name, status indicators, etc.) to match your preferences or terminal theme.  It supports loading themes from files.\n",
       "\n",
       "* **Gain more context:**  Beyond basic status, it can display information like the upstream branch (if any) showing you whether you're ahead or behind, and even the username/repository name.\n",
       "\n",
       "In short, it's a tool to make your Git experience more integrated and visually informative within your shell.  It's installed and then automatically updates your prompt each time you start a new command line.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_contents = !curl https://raw.githubusercontent.com/magicmonty/bash-git-prompt/refs/heads/master/gitprompt.sh\n",
    "\n",
    "explain_prompt = f\"\"\"\n",
    "Please explain what this file does at a very high level. What is it, and why would I use it?\n",
    "\n",
    "```\n",
    "{file_contents}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "\n",
    "response = model.generate_content(explain_prompt, request_options=retry_policy)\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5603415c",
   "metadata": {},
   "source": [
    "<span style=\"font-size:18px; color:rgba(0, 0, 0, 0.5);\">Learn more</span>\n",
    "\n",
    "---\n",
    "To learn more about prompting in depth:\n",
    "\n",
    "* Check out the whitepaper issued with today's content,\n",
    "* Try out the apps listed at the top of this notebook ([TextFX](https://textfx.withgoogle.com/), [SQL Talk](https://sql-talk-r5gdynozbq-uc.a.run.app/) and [NotebookLM](https://notebooklm.google/)),\n",
    "* Read the [Introduction to Prompting](https://ai.google.dev/gemini-api/docs/prompting-intro) from the Gemini API docs,\n",
    "* Explore the Gemini API's [prompt gallery](https://ai.google.dev/gemini-api/prompts) and try them out in AI Studio,\n",
    "* Check out the Gemini API cookbook for [inspirational examples](https://github.com/google-gemini/cookbook/blob/main/examples/) and [educational quickstarts](https://github.com/google-gemini/cookbook/blob/main/quickstarts/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af7c985-a2c6-49c4-8647-deb078dca2f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
